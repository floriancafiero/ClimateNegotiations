{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f4f9f8",
   "metadata": {},
   "source": [
    "# DSPy optimisation pipeline for UNFCCC obstruction detector\n",
    "*Generated 2025-05-20*\n",
    "\n",
    "This notebook loads your gold‐standard labels, splits them into train/dev/test, uses **DSPy** to choose the best subset of few‑shot demonstrations and prompt wording, evaluates accuracy/F1, and saves the resulting prompt as `best_prompt.json`.\n",
    "\n",
    "> **Tip**  Run the cells top‑to‑bottom after installing the requirements.  \n",
    "> Adjust file paths if your CSVs live elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5871e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 1  Install packages (skip if already installed)\n",
    "!pip install --quiet dspy-ai pandas scikit-learn openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c36479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 2  Imports & basic config\n",
    "import os, json, random, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support\n",
    "import dspy\n",
    "from dspy import Argument, Variable, Predict\n",
    "from dspy.optimizers import ExhaustiveFewShot\n",
    "\n",
    "# Set your OpenAI key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'paste-your-key-here')\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79def511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 3  Load labelled data and paragraph texts\n",
    "LABEL_PATH   = 'manual_annotations.csv'  # path to file you downloaded earlier\n",
    "PARAGRAPH_CSV = 'ENB_UNFCCC 1995-2024 - final_data_UNFCCC.csv'  # full corpus\n",
    "\n",
    "df_labels = pd.read_csv(LABEL_PATH)\n",
    "df_texts  = pd.read_csv(PARAGRAPH_CSV, usecols=['index', 'paragraph'])\n",
    "\n",
    "df = df_labels.merge(df_texts, on='index', how='left')\n",
    "assert df.paragraph.notna().all(), \"Some indices not found in paragraph CSV\"\n",
    "\n",
    "print(f\"{len(df)} total labelled rows  |  \"\n",
    "      f\"{(df.obstruction=='y').sum()} positives, \"\n",
    "      f\"{(df.obstruction=='n').sum()} negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aca6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 4  Train / dev / test split (80 / 10 / 10 stratified on obstruction)\n",
    "train_val, test = train_test_split(df, test_size=0.1, stratify=df.obstruction, random_state=42)\n",
    "train, dev      = train_test_split(train_val, test_size=0.1111, stratify=train_val.obstruction, random_state=42)\n",
    "\n",
    "for name, part in [('train', train), ('dev', dev), ('test', test)]:\n",
    "    pos = (part.obstruction=='y').sum()\n",
    "    print(f\"{name:>5}: {len(part):4} rows  |  {pos} positives\")\n",
    "\n",
    "# Helper: record tuples (paragraph text, gold label) for DSPy\n",
    "train_set = [(row.paragraph, row.obstruction) for _, row in train.iterrows()]\n",
    "dev_set   = [(row.paragraph, row.obstruction) for _, row in dev.iterrows()]\n",
    "test_set  = [(row.paragraph, row.obstruction) for _, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 5  Define DSPy program (prompt with variables for rules + demos)\n",
    "SYSTEM_RULES_OPTIONS = [\n",
    "    \"\"\"You are an expert analyst of negotiation tactics in UN climate talks. \n",
    "Return JSON with keys 'obstruction' ('yes'/'no') only.\"\"\",\n",
    "\n",
    "    \"\"\"Classify whether the excerpt contains obstruction to UNFCCC goals. \n",
    "Answer strictly with 'yes' or 'no'.\"\"\",\n",
    "\n",
    "    \"\"\"Determine if this paragraph shows any obstruction tactic as per UNFCCC definitions \n",
    "and reply with 'yes' or 'no'.\"\"\"\n",
    "]\n",
    "\n",
    "def make_demos(df_subset):\n",
    "    demos = []\n",
    "    for _, row in df_subset.iterrows():\n",
    "        demos.append({\n",
    "            \"paragraph\": row.paragraph,\n",
    "            \"label\": \"yes\" if row.obstruction=='y' else \"no\"\n",
    "        })\n",
    "    return demos\n",
    "\n",
    "# Precompute 40 candidate demo subsets of size 20 (balanced roughly)\n",
    "positive_rows = train[train.obstruction=='y']\n",
    "negative_rows = train[train.obstruction=='n']\n",
    "demo_subsets = []\n",
    "for i in range(40):\n",
    "    demos = pd.concat([\n",
    "        positive_rows.sample(min(len(positive_rows),10), replace=False, random_state=100+i),\n",
    "        negative_rows.sample(10, replace=False, random_state=200+i)\n",
    "    ])\n",
    "    demo_subsets.append(make_demos(demos))\n",
    "\n",
    "class ObstructionClassifier(Predict):\n",
    "    paragraph = Argument()\n",
    "    rules     = Variable()\n",
    "    demos     = Variable()\n",
    "\n",
    "    def forward(self, paragraph):\n",
    "        return dspy.chat_completion(\n",
    "            model='gpt-4o-preview',\n",
    "            system=self.rules,\n",
    "            user=f\"Paragraph:\\n\\\"\\\"\\\"\\n{paragraph}\\n\\\"\\\"\\\"\",\n",
    "            few_shot=self.demos,\n",
    "            temperature=0\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 6  Optimise with ExhaustiveFewShot\n",
    "prog = ObstructionClassifier()\n",
    "opt  = ExhaustiveFewShot(max_demos=20)   # searches subsets up to 20\n",
    "\n",
    "best = opt(\n",
    "    program  = prog,\n",
    "    argspace = {\n",
    "        'rules': SYSTEM_RULES_OPTIONS,\n",
    "        'demos': demo_subsets\n",
    "    },\n",
    "    trainset = train_set,\n",
    "    devset   = dev_set,\n",
    "    metric   = lambda y_hat, y: f1_score(y, y_hat, pos_label='y')\n",
    ")\n",
    "\n",
    "print(\"✓ optimisation finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c82bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 7  Evaluate on held‑out test set\n",
    "best_predictions = [best(paragraph=p).prediction.strip().lower() for p, _ in test_set]\n",
    "gold = [label for _, label in test_set]\n",
    "\n",
    "print(classification_report(gold, best_predictions, target_names=['no','yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 8  Save best prompt & demo list\n",
    "prompt_bundle = {\n",
    "    'system_rules': best['rules'],\n",
    "    'demos': best['demos']\n",
    "}\n",
    "with open('best_prompt.json', 'w') as fp:\n",
    "    json.dump(prompt_bundle, fp, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✓ saved best_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✱ 9  How to use the optimised prompt\n",
    "print(json.dumps(prompt_bundle, indent=2)[:1000] + ' ...')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
